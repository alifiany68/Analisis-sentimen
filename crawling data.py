# -*- coding: utf-8 -*-
"""Salinan dari Crawl data twitter > 2000 tweets - 19 Juli 2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LHh7KesyK7ED6ruUy-xo101gcTmfnkzs

# Crawl Data Twitter > 2000 Tweets
The crawling process was done using Tweet-Harvest. Written by Helmi Satria on  March 30th 2024.
"""

#@title Twitter Auth Token

twitter_auth_token = '14edb28e64a5ff53c6aa84bc456ee9d92b42434b' # change this auth token

# Import required Python package
!pip install pandas

# Install Node.js (because tweet-harvest built using Node.js)
!sudo apt-get update
!sudo apt-get install -y ca-certificates curl gnupg
!sudo mkdir -p /etc/apt/keyrings
!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg

!NODE_MAJOR=20 && echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list

!sudo apt-get update
!sudo apt-get install nodejs -y

!node -v

# Crawl Data

filename = 'kaburajadulu_BARU.csv'
search_keyword = 'kaburajadulu since:2024-08-01 until:2025-03-31 lang:id'
limit = 1500

!npx -y tweet-harvest@2.6.1 -o "{filename}" -s "{search_keyword}" --tab "LATEST" -l {limit} --token {twitter_auth_token}

import pandas as pd

# Specify the path to your CSV file
file_path = f"tweets-data/{filename}"

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_path, delimiter=",")

# Display the DataFrame
display(df)

# Cek jumlah data yang didapatkan

num_tweets = len(df)
print(f"Jumlah tweet dalam dataframe adalah {num_tweets}.")